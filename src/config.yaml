# src/config.yaml

# --- Global Settings ---
project_name: "LSTM-RoBERTa-Sentiment"
results_dir: "results"  # Directory for plots, logs, etc.
models_dir: "models"    # Directory for saved models
random_state: 42
database_path: "data/tweets_dataset.db"       # New path (inside src/data/)



# --- Data Settings ---
# Overall split for final train/test
test_size: 0.2
# Split of the initial training data used for HPO validation
hpo_validation_size: 0.25

# --- LSTM-RoBERTa Settings ---
lstm_roberta:
  # Model Architecture Defaults (used if not tuned or for final train)
  roberta_model: "roberta-base" # Base transformer
  embedding_dim: 768 # Typically matches RoBERTa hidden size
  hidden_dim: 256
  num_layers: 2
  dropout: 0.3
  num_attention_heads: 4
  use_layer_norm: True
  use_residual: True
  use_pooler_output: False
  freeze_roberta: True # Default freeze setting

  # Training Defaults (can be overridden by args or Optuna)
  learning_rate: 2e-5
  weight_decay: 0.01
  batch_size: 16
  max_length: 128
  final_epochs: 20 # Epochs for the final training run after HPO
  use_scheduler: True
  scheduler_warmup_steps: 100
  gradient_accumulation_steps: 2
  max_grad_norm: 1.0

  # Data Loading Settings
  base_sample_size: 200000 # Default sample size loaded initially

  # Optuna Hyperparameter Optimization (HPO) Settings
  hpo:
    enabled: true # Set to false to skip HPO and use defaults/args
    n_trials: 50 # Number of Optuna trials to run
    timeout_per_trial: 3600 # Timeout in seconds for a single HPO trial (optional)
    hpo_sample_size: 10000 # Sample size used *during* HPO (subset of base_sample_size)
    hpo_epochs: 10 # Epochs *per HPO trial* (usually fewer than final_epochs)
    # --- Search Space ---
    # Optuna will suggest values within these ranges/sets
    search_space:
      learning_rate: { type: "float", low: 1e-6, high: 5e-5, log: True }
      hidden_dim: { type: "categorical", choices: [128, 256, 512] }
      num_layers: { type: "int", low: 1, high: 3 }
      dropout: { type: "float", low: 0.1, high: 0.5 }
      num_attention_heads: { type: "categorical", choices: [2, 4, 8] }
      weight_decay: { type: "float", low: 0.0, high: 0.1 }
      # freeze_roberta: { type: "categorical", choices: [True, False] } # Optional: tune freezing

# --- HAN-RoBERTa Settings ---
han_roberta: 
  # Model Architecture Defaults
  roberta_model: "roberta-base"
  hidden_dim: 256 # Dimension for the GRU/LSTM layer after RoBERTa
  num_layers: 1   # Number of GRU/LSTM layers
  dropout: 0.3
  use_gru: True   # Use GRU (True) or LSTM (False) after RoBERTa
  use_layer_norm: True
  freeze_roberta: True # Default freeze setting

  # Training Defaults
  learning_rate: 2e-5
  weight_decay: 0.01
  batch_size: 16
  max_length: 128
  final_epochs: 20 # Epochs for the final training run after HPO
  use_scheduler: True
  scheduler_warmup_steps: 100
  gradient_accumulation_steps: 2
  max_grad_norm: 1.0

  # Data Loading Settings
  base_sample_size: 200000 # Default sample size loaded initially

  # Optuna Hyperparameter Optimization (HPO) Settings
  hpo:
    enabled: true # Set to false to skip HPO and use defaults/args
    n_trials: 50 # Number of Optuna trials to run
    timeout_per_trial: 3600 # Timeout in seconds for a single HPO trial (optional)
    hpo_sample_size: 10000 # Sample size used *during* HPO (subset of base_sample_size)
    hpo_epochs: 10 # Epochs *per HPO trial*
    # --- Search Space (Adjust based on HAN-RoBERTa parameters) ---
    search_space:
      learning_rate: { type: "float", low: 1e-6, high: 5e-5, log: True }
      hidden_dim: { type: "categorical", choices: [128, 256, 384] } # Adjusted choices
      num_layers: { type: "int", low: 1, high: 2 } # Often 1 or 2 layers for RNN on top
      dropout: { type: "float", low: 0.1, high: 0.5 }
      weight_decay: { type: "float", low: 0.0, high: 0.1 }
      # use_gru: { type: "categorical", choices: [True, False] } # Optional: tune GRU vs LSTM
      # freeze_roberta: { type: "categorical", choices: [True, False] } # Optional: tune freezing

# --- GNN-RoBERTa Settings --- # 
gnn_roberta:
  # Model Architecture Defaults
  roberta_model: "roberta-base"
  gnn_out_features: 128 # Output features per GAT head
  gnn_heads: 4          # Number of GAT heads
  gnn_layers: 1         # Number of GAT layers (SimpleGATLayer)
  dropout: 0.3
  use_layer_norm: True
  freeze_roberta: True # Default freeze setting

  # Training Defaults
  learning_rate: 2e-5
  weight_decay: 0.01
  batch_size: 16
  max_length: 128
  final_epochs: 20 # Epochs for the final training run after HPO
  use_scheduler: True
  scheduler_warmup_steps: 100
  gradient_accumulation_steps: 2
  max_grad_norm: 1.0

  # Data Loading Settings
  base_sample_size: 200000 # Default sample size loaded initially

  # Optuna Hyperparameter Optimization (HPO) Settings
  hpo:
    enabled: true # Set to false to skip HPO and use defaults/args
    n_trials: 50 # Number of Optuna trials to run
    timeout_per_trial: 3600 # Timeout in seconds for a single HPO trial (optional)
    hpo_sample_size: 10000 # Sample size used *during* HPO (subset of base_sample_size)
    hpo_epochs: 10 # Epochs *per HPO trial*
    # --- Search Space (Adjust based on GNN-RoBERTa parameters) ---
    search_space:
      learning_rate: { type: "float", low: 1e-6, high: 5e-5, log: True }
      gnn_out_features: { type: "categorical", choices: [64, 128, 256] } # Tune GAT output dim per head
      gnn_heads: { type: "categorical", choices: [2, 4, 8] } # Tune number of GAT heads
      gnn_layers: { type: "int", low: 1, high: 2 } # Tune number of GAT layers
      dropout: { type: "float", low: 0.1, high: 0.5 }
      weight_decay: { type: "float", low: 0.0, high: 0.1 }
      # freeze_roberta: { type: "categorical", choices: [True, False] } # Optional: tune freezing
